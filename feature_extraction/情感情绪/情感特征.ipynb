{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æƒ…æ„Ÿææ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\qy201\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.464 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æƒ…æ„Ÿåˆ†æç»“æœå·²ç»æˆåŠŸä¿å­˜ä¸º E:/comorbidity/feature_extraction/æƒ…æ„Ÿæƒ…ç»ª/BOSONæƒ…æ„Ÿåˆ†æç»“æœ.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "df = pd.read_excel(\"E:/comorbidity/data/final_data.xlsx\")\n",
    "# åŠ è½½æƒ…æ„Ÿè¯å…¸ï¼ˆä»æ–‡ä»¶åŠ è½½ï¼‰\n",
    "def load_word_list(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        words = set([line.strip() for line in f.readlines()])\n",
    "    return words\n",
    "\n",
    "# å‡è®¾æ–‡ä»¶è·¯å¾„æ˜¯ç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„ï¼Œæ›¿æ¢ä¸ºä½ çš„å®é™…æ–‡ä»¶è·¯å¾„\n",
    "positive_words_path = \"E:/comorbidity/feature_extraction/æƒ…æ„Ÿæƒ…ç»ª/æƒ…æ„Ÿææ€§è¯å…¸/æ­£é¢æƒ…ç»ªè¯.txt\"  # æ›¿æ¢ä¸ºæ­£å‘è¯å…¸æ–‡ä»¶è·¯å¾„\n",
    "negative_words_path =\"E:/comorbidity/feature_extraction/æƒ…æ„Ÿæƒ…ç»ª/æƒ…æ„Ÿææ€§è¯å…¸/è´Ÿé¢æƒ…ç»ªè¯.txt\"  # æ›¿æ¢ä¸ºè´Ÿå‘è¯å…¸æ–‡ä»¶è·¯å¾„\n",
    "\n",
    "# åŠ è½½æ­£å‘å’Œè´Ÿå‘æƒ…æ„Ÿè¯æ±‡\n",
    "positive_words = load_word_list(positive_words_path)\n",
    "negative_words = load_word_list(negative_words_path)\n",
    "\n",
    "# æ‰‹åŠ¨æ·»åŠ æ­£å‘å’Œè´Ÿå‘æƒ…æ„Ÿè¯æ±‡\n",
    "manual_positive_words = {\"å¥½å¼€å¿ƒ\", \"é«˜å…´\", \"å–œæ¬¢\", \"æ„‰å¿«\", \"çƒ­çˆ±\", \"ç¥ç¦\", \"ç¾å¥½\",'å¼€æ£®','å¿«ä¹','å¸Œæœ›',\n",
    "                        'å¥½çœ‹','èŠ±èŠ±','å¿«ä¹','åˆ†äº«','è½æ—¥','çœŸå¥½','éŸ³ä¹','æ—¥å¸¸','ç¿çƒ‚','ç¤¼ç‰©','é¡ºé‚',\n",
    "                        'å¤ªé˜³','å¥½åƒ','é£æ™¯','ç²‰å«©','å°çŒ«å’ª','ç¾æ–‡','å¥½äº‹','åŠ æ²¹'}\n",
    "manual_negative_words = {\"å¤±æœ›\", \"éš¾è¿‡\", \"ç—›è‹¦\", \"æ‚²ä¼¤\", \"ä¸æ»¡\", \"æ„¤æ€’\", \"æ²®ä¸§\",\"è¯\",\"å–‰å’™\",\"å¹²å‘•\",\"è‡ªå‘\",\"æ‡¦å¼±\",\"ä½é™¢\",\n",
    "                        \"æŠ‘éƒ\",\"å´©æºƒ\",\"å€¾è¯‰\",\"ä¸¥é‡\",\"è¯ç‰©\",\"å¥½ç´¯\",\"å‹åŠ›\",\"ä¸å¼€å¿ƒ\",'ç„¦è™‘','åµ','ç¡ä¸ç€','æŠ±æŠ±','å¤±æœ›',\n",
    "                        'æ·¡æ·¡çš„','å¥½ç´¯','ç¡ä¸ç€','åŒ»é™¢','æ€€ç–‘','çƒ‚é€','çŠ¯ç—…','é¼ æ‰','å¥½ç´¯','å°é—­','å™©æ¢¦','æ´»ç€',\n",
    "                        'ç¦»èŒ','èƒ¸é—·','æ­»å»','èº¯ä½“åŒ–','æƒ³æ­»','å¿ƒæ…Œ','åˆæ˜¯','å¤±çœ ','æƒ³å“­','æŠ±æŠ±','å€’éœ‰','å¥½éš¾','ç™¾èˆ¬åˆéš¾',\n",
    "                        'ç³Ÿç³•','æˆ‘æ¨','å­¤ç‹¬','å¥½éš¾','æ°”æ­»æˆ‘äº†','å¯»æ±‚','åŒç›¸','è½»åº¦','å¾—ç—…','åƒè¯','ç¡ä¸ç€','æƒ³å“­'}\n",
    "\n",
    "# å°†æ‰‹åŠ¨è¯æ±‡æ·»åŠ åˆ°è¯å…¸ä¸­\n",
    "positive_words.update(manual_positive_words)  # å°†æ‰‹åŠ¨æ­£å‘è¯æ±‡åŠ å…¥æ­£å‘æƒ…æ„Ÿè¯å…¸\n",
    "negative_words.update(manual_negative_words)  # å°†æ‰‹åŠ¨è´Ÿå‘è¯æ±‡åŠ å…¥è´Ÿå‘æƒ…æ„Ÿè¯å…¸\n",
    "\n",
    "# è‡ªå®šä¹‰åˆ†è¯å’Œæƒ…æ„Ÿåˆ†æå‡½æ•°\n",
    "def sentiment_analysis(text):\n",
    "    if not text:  # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ºç©º\n",
    "        return 'ä¸­æ€§'\n",
    "\n",
    "    words = jieba.lcut(str(text))\n",
    "    pos_count = len([w for w in words if w in positive_words])\n",
    "    neg_count = len([w for w in words if w in negative_words])\n",
    "    \n",
    "    if pos_count > neg_count:\n",
    "        return 'æ­£å‘'\n",
    "    elif neg_count > pos_count:\n",
    "        return 'è´Ÿå‘'\n",
    "    else:\n",
    "        return 'ä¸­æ€§'\n",
    "\n",
    "\n",
    "# åº”ç”¨æƒ…æ„Ÿåˆ†æ\n",
    "df['BOSONæƒ…æ„Ÿåˆ†æç»“æœ'] = df['åˆ†è¯æ–‡æœ¬'].apply(sentiment_analysis)\n",
    "\n",
    "# åªä¿å­˜åºå·å’Œæƒ…æ„Ÿåˆ†æç»“æœ\n",
    "result_df = df[['åºå·', 'BOSONæƒ…æ„Ÿåˆ†æç»“æœ']]\n",
    "\n",
    "# ä¿å­˜ä¸º Excel æ–‡ä»¶\n",
    "output_path = 'E:/comorbidity/feature_extraction/æƒ…æ„Ÿæƒ…ç»ª/BOSONæƒ…æ„Ÿåˆ†æç»“æœ.xlsx'  # è¾“å‡ºçš„ Excel æ–‡ä»¶è·¯å¾„\n",
    "result_df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"æƒ…æ„Ÿåˆ†æç»“æœå·²ç»æˆåŠŸä¿å­˜ä¸º {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOSONæƒ…æ„Ÿåˆ†æç»“æœ\n",
      "è´Ÿå‘    48380\n",
      "æ­£å‘    23441\n",
      "ä¸­æ€§    23342\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹æ¯ç§æƒ…æ„Ÿåˆ†æç»“æœçš„å‡ºç°æ¬¡æ•°\n",
    "sentiment_counts = df['BOSONæƒ…æ„Ÿåˆ†æç»“æœ'].value_counts()\n",
    "\n",
    "# æ‰“å°ç»“æœ\n",
    "print(sentiment_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‰100ä¸ªé«˜é¢‘è¯åŠå…¶é¢‘ç‡ï¼š\n",
      "        è¯è¯­    é¢‘æ¬¡\n",
      "1079   æŠ‘éƒç—‡  4302\n",
      "6       æ²¡æœ‰  3765\n",
      "189     çœŸçš„  2409\n",
      "104     æ„Ÿè§‰  2171\n",
      "98      å¯ä»¥  2095\n",
      "...    ...   ...\n",
      "22579   ##   402\n",
      "11      ä¸å¥½   398\n",
      "1012    æœ€å   388\n",
      "483     æ¯æ¬¡   388\n",
      "1542    å®¹æ˜“   386\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from collections import Counter\n",
    "\n",
    "filtered_df = df[(df['auto_label'].isin(['æŠ‘éƒ', 'ç„¦è™‘', 'æŠ‘éƒç„¦è™‘'])) & (df['BOSONæƒ…æ„Ÿåˆ†æç»“æœ'] == 'æ­£å‘')]\n",
    "# å°†ç­›é€‰åçš„åˆ†è¯æ–‡æœ¬åˆå¹¶ä¸ºä¸€ä¸ªé•¿å­—ç¬¦ä¸²\n",
    "all_text = ' '.join(filtered_df['åˆ†è¯æ–‡æœ¬'].dropna().astype(str))  # åˆå¹¶æ–‡æœ¬å¹¶å»æ‰ç©ºå€¼\n",
    "\n",
    "# å°†åˆå¹¶çš„æ–‡æœ¬æŒ‰ç©ºæ ¼åˆ†å‰²æˆå•è¯åˆ—è¡¨\n",
    "words = all_text.split()  # ä½¿ç”¨ç©ºæ ¼åˆ†éš”è¯è¯­\n",
    "\n",
    "# ç»Ÿè®¡è¯é¢‘\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# å°†è¯é¢‘ç»“æœè½¬æ¢ä¸º DataFrame\n",
    "word_freq_df = pd.DataFrame(word_counts.items(), columns=['è¯è¯­', 'é¢‘æ¬¡'])\n",
    "\n",
    "# æŒ‰é¢‘æ¬¡é™åºæ’åº\n",
    "word_freq_df = word_freq_df.sort_values(by='é¢‘æ¬¡', ascending=False)\n",
    "\n",
    "# æ‰“å°å‰100ä¸ªé«˜é¢‘è¯åŠå…¶é¢‘æ¬¡\n",
    "print(\"å‰100ä¸ªé«˜é¢‘è¯åŠå…¶é¢‘ç‡ï¼š\")\n",
    "print(word_freq_df.head(100))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    å‘å¸–å†…å®¹    æƒ…æ„Ÿåˆ†æåˆ†æ•°\n",
      "0      æˆ‘ä¸çŸ¥é“æˆ‘æ˜¯æ€ä¹ˆäº†ã€‚ä¸€å¼€å§‹ç©ç‹è€…çš„æ—¶å€™ï¼Œè§‰å¾—è¿™åªæ˜¯ä¸ªæ¸¸æˆè€Œå·²æ²¡å¿…è¦å¤ªåœ¨ä¹è¾“èµ¢ï¼Œå¼€å¿ƒå°±å¥½ï¼Œå½“æ—¶...  0.799442\n",
      "1                                               å¿ƒæƒ…ç³Ÿç³•ï¼Œæ™šå®‰å§  0.448440\n",
      "2                                     æˆ‘çœŸçš„å¥½æƒ³æ­»å•Šï¼Œæˆ‘æ´»ä¸ä¸‹å»äº†ï¼Œå¥½ç—›è‹¦  0.656004\n",
      "3      Bç«™çœŸçš„æ˜¯å¤ªæ‡‚æˆ‘äº†ï¼Œæˆ‘åˆ·åˆ°è¿™æ¡è§†é¢‘ï¼Œè¯´çš„æ˜¯æŠ¥å¤æ€§ç†¬å¤œï¼Œè§†é¢‘è¯´çš„æ˜¯ç™½å¤©ä¸€ç›´ä¸Šç­ï¼Œåˆ°äº†æ™šä¸Šå°±å¼€å§‹...  1.000000\n",
      "4                      æ˜å¤©å°±æ”¾å‡å•¦ï¼ŒåŒæ‹…ä¹°äº†åå¤©çš„é«˜é“ç¥¨æ¥æ‰¾æˆ‘ç©ï¼Œæ²¡æœ‰å¥½æœ‹å‹æˆ‘å¯æ€ä¹ˆåŠå•Š  0.077196\n",
      "...                                                  ...       ...\n",
      "95158        è¯·é—®æ¯æ¬¡ç´§å¼ å°±ä¼šå–‰å’™å‘ç´§ç„¶åä¸€ç›´åå£æ°´ï¼Œç„¶åå°±ä¼šæœ‰è¦åå¹²å‘•çš„æ„Ÿè§‰åº”è¯¥æ€ä¹ˆç¼“è§£æˆ–è€…è§£å†³å‘¢  0.031647\n",
      "95159  åœ¨è·Ÿæœ‹å‹è·¨å¹´ï¼Œç°åœ¨è¿˜æ²¡å›å®¶ï¼Œçˆ¸çˆ¸æ‰“ç”µè¯ğŸ¤³ï¼Œæˆ‘æ²¡æ¥ï¼Œç„¶åæ‰“ç»™å§‘å§‘ï¼Œæˆ‘åˆšåˆšå›äº†ï¼Œå¯¹çˆ¸çˆ¸çš„æ€åº¦ä¸å¤ª...  0.032254\n",
      "95160             å®å„¿ä»¬è¯·é—®ç´§å¼ å°±ä¼šå–‰å’™å‘ç´§ä¸€ç›´åå£æ°´è¯¥æ€ä¹ˆç¼“è§£å•Šï¼Œæˆ‘ä¸€ç›´è¿™æ ·å°±ä¼šè§‰å¾—æœ‰ç‚¹æƒ³å‘•  0.602074\n",
      "95161                                    2024 è¦å‹‡æ•¢å’Œè¿™é‡Œè¯´å†è§ï¼  0.820228\n",
      "95162                             å„ä½ç—…å‹ï¼Œæ–°å¹´å¿«ä¹ï¼Œæ–°ä¸€å¹´è¦å¤šé¼“åŠ±ä¸€ä¸‹è‡ªå·±ã€‚  0.957001\n",
      "\n",
      "[95163 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##snownlpæ•ˆç‡ä½ä¸‹ï¼Œæ‰€ä»¥è¿™éƒ¨åˆ†ç‰¹å¾ä¸ç”¨\n",
    "import pandas as pd\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "# å®šä¹‰æƒ…æ„Ÿåˆ†æå‡½æ•°ï¼Œè¿”å›æƒ…æ„Ÿåˆ†æ•°\n",
    "def analyze_sentiment_score(text):\n",
    "    if pd.isna(text) or str(text).strip() == '':  # å¤„ç†ç©ºå€¼\n",
    "        return None  # ç©ºæ–‡æœ¬è¿”å› None\n",
    "    s = SnowNLP(str(text))\n",
    "    score = s.sentiments  # è·å–æƒ…æ„Ÿåˆ†æ•°\n",
    "    return score\n",
    "\n",
    "# åº”ç”¨æƒ…æ„Ÿåˆ†æï¼Œè¿”å›æƒ…æ„Ÿåˆ†æ•°\n",
    "df['æƒ…æ„Ÿåˆ†æåˆ†æ•°'] = df['å‘å¸–å†…å®¹'].apply(analyze_sentiment_score)\n",
    "\n",
    "# æŸ¥çœ‹ç»“æœ\n",
    "print(df[['å‘å¸–å†…å®¹', 'æƒ…æ„Ÿåˆ†æåˆ†æ•°']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æƒ…æ„Ÿåˆ†æç»“æœå·²ç»æˆåŠŸä¿å­˜ä¸º E:/comorbidity/feature_extraction/æƒ…æ„Ÿæƒ…ç»ª/SnowNLPæƒ…æ„Ÿåˆ†æåˆ†æ•°.xlsx\n"
     ]
    }
   ],
   "source": [
    "# åªä¿å­˜åºå·å’Œæƒ…æ„Ÿåˆ†æç»“æœ\n",
    "result_df = df[['åºå·', 'æƒ…æ„Ÿåˆ†æåˆ†æ•°']]\n",
    "\n",
    "# ä¿å­˜ä¸º Excel æ–‡ä»¶\n",
    "output_path = 'E:/comorbidity/feature_extraction/æƒ…æ„Ÿæƒ…ç»ª/SnowNLPæƒ…æ„Ÿåˆ†æåˆ†æ•°.xlsx'  # è¾“å‡ºçš„ Excel æ–‡ä»¶è·¯å¾„\n",
    "result_df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"æƒ…æ„Ÿåˆ†æç»“æœå·²ç»æˆåŠŸä¿å­˜ä¸º {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æƒ…æ„Ÿå¼ºåº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æƒ…æ„Ÿå¼ºåº¦: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###æƒ…æ„Ÿå¼ºåº¦è®¡ç®—ç¤ºä¾‹\n",
    "import re\n",
    "\n",
    "# å®šä¹‰è´Ÿé¢æƒ…ç»ªæç«¯è¡¨è¾¾çš„å…³é”®è¯åˆ—è¡¨\n",
    "negative_keywords = [\n",
    "    \"å®Œå…¨æ— æ³•å¿å—\", \"å½»åº•å¤±è´¥\", \"æå…¶ç—›è‹¦\", \"ç»æœ›è‡³æ\", \"ç”Ÿä¸å¦‚æ­»\", \"æ— æ³•æ‰¿å—\"\n",
    "]\n",
    "\n",
    "# æ–‡æœ¬é¢„å¤„ç†å‡½æ•°\n",
    "def preprocess_text(text):\n",
    "    # å»é™¤æ ‡ç‚¹ç¬¦å·\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # è½¬æ¢ä¸ºå°å†™å­—æ¯\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# è®¡ç®—æƒ…ç»ªå¼ºåº¦å‡½æ•°\n",
    "def calculate_emotion_intensity(text):\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    intensity = 0\n",
    "    for keyword in negative_keywords:\n",
    "        if keyword in preprocessed_text:\n",
    "            intensity += 1\n",
    "    return intensity\n",
    "\n",
    "# ç¤ºä¾‹æ–‡æœ¬\n",
    "text = \"æˆ‘æ„Ÿè§‰å®Œå…¨æ— æ³•å¿å—è¿™ç§æƒ…å†µï¼Œè¿™ç®€ç›´æ˜¯å½»åº•å¤±è´¥ã€‚\"\n",
    "\n",
    "# è®¡ç®—æƒ…æ„Ÿå¼ºåº¦\n",
    "emotion_intensity = calculate_emotion_intensity(text)\n",
    "print(f\"æƒ…æ„Ÿå¼ºåº¦: {emotion_intensity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²æˆåŠŸå°†ç»“æœå¯¼å‡ºåˆ°æƒ…æ„Ÿå¼ºåº¦.csv æ–‡ä»¶ä¸­ã€‚\n"
     ]
    }
   ],
   "source": [
    "##è´Ÿé¢å…³é”®è¯ä¸èµ‹äºˆæƒé‡\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "# å®šä¹‰è´Ÿé¢æƒ…ç»ªæç«¯è¡¨è¾¾çš„å…³é”®è¯åˆ—è¡¨\n",
    "negative_keywords = [\n",
    "\n",
    "       # \"å®Œå…¨æ— æ³•å¿å—\", \"å½»åº•å¤±è´¥\", \"æå…¶ç—›è‹¦\", \"ç»æœ›è‡³æ\", \"ç”Ÿä¸å¦‚æ­»\", \"æ— æ³•æ‰¿å—\",''\n",
    "    'å¾ˆ', \"éå¸¸\", \"çœŸçš„\", \"ç®€ç›´\", \"ç»\", \"æ— æ³•\",'å½»åº•','æ­»','æœ€','ä¸','åˆ','å“­','æ²¡æœ‰','å´©æºƒ','è¯','è‡ªæ€','å‰²è…•','è¶…çº§','æ´»','ç»æœ›',\n",
    "    'ä¸¥é‡','å­¤ç‹¬','è‘¬'\n",
    "]\n",
    "\n",
    "# æ–‡æœ¬é¢„å¤„ç†å‡½æ•°\n",
    "def preprocess_text(text):\n",
    "    # å»é™¤æ ‡ç‚¹ç¬¦å·\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # è½¬æ¢ä¸ºå°å†™å­—æ¯\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# è®¡ç®—æƒ…ç»ªå¼ºåº¦å‡½æ•°\n",
    "def calculate_emotion_intensity(text):\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    intensity = 0\n",
    "    for keyword in negative_keywords:\n",
    "        if keyword in preprocessed_text:\n",
    "            intensity += 1\n",
    "    return intensity\n",
    "\n",
    "# å­˜å‚¨ç»“æœçš„åˆ—è¡¨\n",
    "results = []\n",
    "\n",
    "# ä»txtæ–‡ä»¶ä¸­è¯»å–æ–‡æœ¬ï¼Œæ¯è¡Œä½œä¸ºä¸€æ¡è®°å½•\n",
    "with open(\"E:/comorbidity/data/output.txt\", 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# åˆ†åˆ«è®¡ç®—æ¯æ¡è®°å½•çš„æƒ…ç»ªå¼ºåº¦ï¼Œå¹¶ä¿å­˜åˆ°ç»“æœåˆ—è¡¨ä¸­\n",
    "for index, line in enumerate(lines, 1):\n",
    "    emotion_intensity = calculate_emotion_intensity(line.strip())\n",
    "    results.append({\n",
    "        \"æ–‡æ¡£åºå·\": index,\n",
    "        \"æƒ…æ„Ÿå¼ºåº¦\": emotion_intensity\n",
    "    })\n",
    "\n",
    "# å°†ç»“æœå†™å…¥csvæ–‡ä»¶\n",
    "with open('E:/comorbidity/feature_extraction/æƒ…æ„Ÿæƒ…ç»ª/æƒ…æ„Ÿå¼ºåº¦.csv', 'w', encoding='utf-8', newline='') as csvfile:\n",
    "    fieldnames = [\"æ–‡æ¡£åºå·\",\"æƒ…æ„Ÿå¼ºåº¦\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for result in results:\n",
    "        writer.writerow(result)\n",
    "\n",
    "print(\"å·²æˆåŠŸå°†ç»“æœå¯¼å‡ºåˆ°æƒ…æ„Ÿå¼ºåº¦.csv æ–‡ä»¶ä¸­ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiuyan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
